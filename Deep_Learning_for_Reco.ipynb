{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reminder Part II: Representation Learning for Recommendation\n",
    "\n",
    "### Similarity-based methods \n",
    "\n",
    "Represent item/user as a feature vector\n",
    "\n",
    "Example: item-to-item collaborative filtering, item is a vector of co-occurence with other items\n",
    "\n",
    "### Dimensionality reduction methods\n",
    "\n",
    "Represent item/user as a vector in shared latent space\n",
    "\n",
    "Example: user-item matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "4ac6dfbf-17af-4d1b-8c0b-9d12578f3ad7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part III: Representation Learning by Deep Learning\n",
    "\n",
    "___\n",
    "\n",
    "Deep learning methods learn hierarchical distributed representations from the data. They have been very successful in solving perception problems such as vision, nlp, speech recognition. Increasingly, deep learning is applied to other problems, such as search and recommendation. \n",
    "\n",
    "\n",
    "![LeNet](fig/lenet.png)\n",
    "![Surf](fig/surf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part III: Overview\n",
    "\n",
    "### Section 1: Introduction to deep learning\n",
    "\n",
    "### Section 2: Multi-layer neural networks\n",
    "\n",
    "* Practical exercise: deep networks for recommendation\n",
    "\n",
    "### Section 3: Item and user representations\n",
    "\n",
    "* Learning item representation with Convolutional Neural Networks\n",
    "* Practical exercise: adding movie poster in deep network for recommendation\n",
    "* Learning user representation with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "bf1f28fe-0e39-41fd-905d-226f982962c5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 1: Introduction to deep learning\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "Logistic regression maps _input_ $x$ to _binary target_ $y$ using _weights_ vector $w$ and _bias_ $b$:\n",
    "\n",
    "$\\hat{y} = \\frac{1}{1+\\exp(-(xw + b))}$ \n",
    "\n",
    "$x \\in R^{d_{in}}, y \\in (0,1), w \\in R^{d_{in}}, b \\in R^{1}$.\n",
    "\n",
    "## Recommendation case\n",
    "\n",
    "**Input**, $x$, is a vector of features that describes user and candidate item\n",
    "\n",
    "For example, user favorite movie genres, candidate movie genre, candidate movie release date.\n",
    "\n",
    "**Target** $y$, is whether user will take action on this candidate item.\n",
    "\n",
    "For example, watch entire movie, only watch a trailer, not watch at all.\n",
    "\n",
    "\n",
    "## Generalized linear model\n",
    "\n",
    "GLM is an extention of linear model to handle other types of $y$ that can be binary-valued, real-valued, categorical-valued:\n",
    "\n",
    " 1. Compute pre-activations: $a(x) = xW + b$\n",
    "\n",
    " 2. Apply **activation function** $o$ to result of 1. \n",
    "\n",
    "$x \\in R^{d_{in}}, y \\in R^{d_{out}}, W \\in R^{d_{in}\\times d_{out}}, b \\in R^{d_{out}}$.\n",
    "\n",
    "#### Activation functions\n",
    "\n",
    "In the case of logistic regression, it's a sigmoid activation function: $o(x) = sigmoid(a(x)) = \\frac{1}{1+\\exp(-a(x))}$\n",
    "\n",
    "Other activation functions: softmax, tanh, ReLU.\n",
    "\n",
    "![Linear regression](fig/1_neuron_explained.png)\n",
    "\n",
    "#### Graphical model\n",
    "\n",
    "GLM can be seen as a neural network with a single neuron.\n",
    "\n",
    "![Linear classifier](fig/linear_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation learning\n",
    "\n",
    "GLMs learn specific task over a feature vector describing the input data. \n",
    "\n",
    "It is well-known that input data representation is crucial for machine learning algorithms. Most ML systems use manually engineered features.\n",
    "\n",
    "Although feature engineering results in performant models, hand-crafting features is time-consuming, brittle and incomplete.\n",
    "\n",
    "Deep neural networks set an ambitious goal to **learn both specific tasks and data representation automatically**.\n",
    "\n",
    "To learn data representation, deep networks make two assumptions. \n",
    "\n",
    "### Composionality hypothesis: representations are composable / form a hierarchy\n",
    "\n",
    "We can find hierarchical structures in images, text, speech, computer programs.\n",
    "\n",
    "![Face detection learned features](fig/face_detection_layers.png) \n",
    "\n",
    "### Distributional hypothesis: representations are distributed, features are not mutually exclusive\n",
    "\n",
    "The input is represented by the activation of a set of features that are not mutually exclusive (think bit vector vs. one-hot encoded vector). It can be exponentially more efficient.\n",
    "\n",
    "![Distributed vs local representation](fig/distributed_vs_local.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "4726501d-338c-4d75-86b7-4dd83b84103e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep networks\n",
    "\n",
    "Deep networks use GLMs as building blocks to implement the composionality property of the representations. They recursively apply the generalized linear predictors on the top of each other. \n",
    "\n",
    "Stacking those predictors forms multiple layers that attempt to learn representations of data with multiple levels of abstraction.\n",
    "\n",
    "These networks are called _**multi-layer neural networks**_ or _**deep networks**_.\n",
    "\n",
    "> Deep network is a composition of functions - matrix multiplication and non-linear transformation:\n",
    "\n",
    ">$\\mu_l(z) = f_l(zW^{(l)})$\n",
    "\n",
    "> $DNN(x) = \\mu_L \\circ \\mu_{L-1} \\circ ... \\circ \\mu_0(x)$\n",
    "\n",
    "![2 Layer NN](fig/nn_shallow_vs_deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantage of deep architectures\n",
    "\n",
    "### Modeling capacity of a single hidden layer neural network\n",
    "\n",
    "* Universal approximation theorem (Homik, '91): single hidden layer neural network can approximate any continuous function arbitrarily well given that there is enough hidden units\n",
    "* Intuition: Taylor series\n",
    "* The size of hidden layer can get arbitrarily large\n",
    "\n",
    "### Efficiency of deep architectures\n",
    "\n",
    "* Before deep networks, there was a relative success with having a single hidden layer (Kernel machines)\n",
    "\n",
    "$y = \\sum_i \\alpha_i K(X, X_i)$\n",
    "\n",
    "$y = \\mu_1 \\circ \\mu_0(x)$\n",
    "\n",
    "* Depending on the kernel function, the number of terms needed to approximate a reasonable function can get arbitrarily large\n",
    "* One can show that some functions that would require exponentially many hidden units in single hidden layer neural network, can be compactly represented with deep neural networks (see [Montufar, 2014](http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks))\n",
    "* The compositional structure enables them to re-use pieces of computation exponentially often in terms of the network's depth\n",
    "$y = \\mu_L \\circ \\mu_{L-1} \\circ ... \\circ \\mu_0(x)$\n",
    "* Think time-space tradeoff in computer programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantage of distributed representations\n",
    "\n",
    "* Hidden units can be active at the same time\n",
    "* Distributed representation is exponentially more efficient than local representation\n",
    "    * In local representation (e.g. clustering), a vector of size $N$ can be represent $N$ regions    \n",
    "    * In distributed representation, a vector of size $N$ can already represent $2^N$ distinct regions (assuming binary values)\n",
    "* This allows to generalize non-locally to near seen regions \n",
    "\n",
    "![Distributed vs local representation](fig/local_vs_distributed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "315a0cd0-6421-4db3-af67-eb4d1e98b8b0"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of deep networks\n",
    "\n",
    "* Model complex non-linear functions efficiently: using hierarchy of layers and distributed representation\n",
    "* Layers act as trainable feature creators: automatic feature engineering\n",
    "* Modularity: a family of methods applicable to a broad class of problems\n",
    "* Model end-to-end systems: go from image to text caption directly, without intermediate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "47c436ce-278a-4ba1-a896-07829eaeb963"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages\n",
    "\n",
    "* Learning deep network is tricky and involves optimizing non-convex functions\n",
    "* Deep networks need more data (and time) to train increased number of parameters and parts of the hierarchy that are far away from data\n",
    "* Network configuration engineering is the new feature engineering   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "968ac6ce-eafb-4723-9eed-1db2b36f0c6a"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Brief history\n",
    "\n",
    "* 50s: Neural networks were invented to model the intuition of how human brain works. \n",
    "* 70s: It took two decades to discover how to efficiently train these models, algorithm called backpropagation. \n",
    "* 90s: LeCun-Net introduces convolutional neural networks. At that time, the datasets were tiny and computers were slow.\n",
    "* 2012s: Deep networks made a come-back in computer vision at ImageNet competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "700bbe29-8cee-41b5-a99c-07d2c93d75d8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2: Multi-layer neural networks\n",
    "\n",
    "Let's take a closer look at a 2 hidden layer neural network. \n",
    "\n",
    "![2 layer NN](fig/2_layer_nn_explained.png)\n",
    "\n",
    "## Model\n",
    "\n",
    "* activation at layer 1 (hidden layer): $h^{(1)} = g(xW^{(1)} + b^{(1)})$\n",
    "* activation at layer 2 (hidden layer): $h^{(2)} = g(h^{(1)}W^{(2)} + b^{(2)})$\n",
    "* output layer: $\\hat{y} = o(g(h^{(2)}W^{(3)} + b^{(3)}))$\n",
    "\n",
    "### Activations\n",
    "\n",
    "* sigmoid activation $g(a) = \\frac{1}{1 + exp(-a)}$\n",
    "* hyperbolic tangent $g(a) = tanh(a)$\n",
    "* ReLU $g(a) = max(a, 0)$\n",
    "    \n",
    "#### Derivatives    \n",
    "* $sigmoid'(a)=sigmoid(a)*(1-sigmoid(a))$\n",
    "* $tanh'(a) = (1 - tanh(a))^2$\n",
    "* $relu'(a) = 1\\ if\\ x \\gt 0, 0\\ otherwise$\n",
    "\n",
    "![Activation functions](fig/activation_functions.png)\n",
    "\n",
    "### Output transformations\n",
    "\n",
    "* sigmoid $o(a) = \\frac{1}{1 + exp(-a)}$ for one-class classification\n",
    "* softmax $o(a) = [\\frac{exp(a_1)}{\\sum_n exp(a_n)},..,\\frac{exp(a_N)}{\\sum_n exp(a_n)}]^{T}$ for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational graph\n",
    "\n",
    "One can see that neural networks induce a computational graph with nodes as computing units on matrices and edges transmitting them across.\n",
    "Computational graph is the main abstration used in deep learning software.\n",
    "\n",
    "![Computational graph](fig/computational_graph.jpg)\n",
    "\n",
    "## Prediction: forward pass\n",
    "\n",
    "NN uses _simple_ operations: multiplication, addition, sigmoid to compute an output vector $y$ based on the input $x$. This is also called a forward pass and it is very efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function\n",
    "\n",
    "Our estimate $\\hat{y} = NeuralNetwork(x, \\theta)$\n",
    "\n",
    "### Classification\n",
    "\n",
    "NN output estimates the probability of the class $c$ given $x$: $\\hat{y} = p(y=c | x)$\n",
    "\n",
    "* negative log-likelihood (cross-entropy loss): $L_{cross-entropy}(\\hat{y}, y) = \\sum_i H(y^{(i)}, \\hat{y^{(i)}}) = - \\sum_i \\sum_c y_c^{(i)} \\log\\hat{y_c^{(i)}}$\n",
    "\n",
    "### Regression \n",
    "\n",
    "* square loss $L_{square}(\\hat{y}, y) = \\sum_i (y^{(i)} - \\hat{y^{(i)}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "b5f07c68-c2a1-41d0-98be-7859ebae74d4"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "We turn the problem of training NN into an optimization problem:\n",
    "\n",
    "$\\theta = \\{ W^{(1)},b^{(1)}, .., W^{(L+1)},b^{(L+1)} \\}$  \n",
    "\n",
    "$arg min_{\\theta} \\frac{1}{N}\\sum_{i} L(NeuralNetwork(x^{(i)}, \\theta), y^{(i)})) + \\lambda Reg(\\theta)$\n",
    "\n",
    "$L$ is a loss function.\n",
    "\n",
    "## Stohastic Gradient Descent (SGD):\n",
    "\n",
    "* Initialize parameters $\\theta = \\{ W^{(1)},b^{(1)}, .., W^{(L+1)},b^{(L+1)} \\}$  \n",
    "* $\\alpha$ is the learning rate\n",
    "* For N iterations  \n",
    "  * For each training example $(x^{(t)}, y^{(t)})$ in **random** order\n",
    "    * $\\Delta = -\\nabla_{\\theta} L(NeuralNetwork(x^{(t)}, \\theta), y^{(t)})) - \\lambda \\nabla_{\\theta} Reg(\\theta)$  \n",
    "    * $\\theta = \\theta + \\alpha \\Delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbpresent": {
     "id": "e867d08d-bf18-49b9-baf2-4e36c9a9491a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "To apply SGD, we need to compute the derivative of the loss function with respect to the weights efficiently. \n",
    "\n",
    "The main mathematical insight is the **chain rule**: $[g(f(x))]'=g'(f(x)) f'(x)$ . \n",
    "\n",
    "It suggests that as long as we know how to write derivatives of the individual functions, we can combine them together in a simple way and computer the derivate for the whole function.\n",
    "\n",
    "**Backpropagation algorithm** is the efficient way to compute the derivative of loss function using chain rule. \n",
    "\n",
    "First, compute prediction using forward pass. Then, backpropogate errors.\n",
    "\n",
    "In practice, software is able to automatically apply backpropagation algorithm for learning a deep network.\n",
    "* [Tensorflow](https://www.tensorflow.org/versions/r0.11/tutorials/index.html)\n",
    "* Torch\n",
    "* Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Good practices\n",
    "\n",
    "### Initialization\n",
    "\n",
    "* initialize weights to small random values\n",
    "\n",
    "### Regularization\n",
    "\n",
    "#### L2 regularization\n",
    "Modify the error function to encourage small $\\theta$\n",
    "    \n",
    "$Error = L(NeuralNetwork(x^{(t)}, \\theta), y^{(t)})) + \\lambda \\sum_k || W^{(k)} ||_2^2$\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "  Remove hidden units stohastically - each hidden unit is to 0 with probability 0.5\n",
    "  \n",
    "  Hidden units cannot co-adapt to other units\n",
    "  \n",
    "  ![Dropout](fig/dropout.png)\n",
    "  \n",
    "### Model selection\n",
    "\n",
    "Hyperparameters: learning rate, initialization, regularization parameter, network configuration\n",
    "\n",
    "* split the data into **training, validaton and test set**\n",
    "* use validation set to select the best configuration\n",
    "\n",
    "### Other\n",
    "\n",
    "* Mini-batches\n",
    "    * compute the gradient of the loss function over a batch of examples (32-128)\n",
    "    * efficient on GPUs\n",
    "* Input normalization\n",
    "    * zero mean, equal variance\n",
    "* Optimization algorithms\n",
    "    * momentum (use previous update directions with a weight)\n",
    "    * per-parameter adaptive learning rates (Agagrad, RMSprop, Adam)        \n",
    "* Track gradient and weights distribution over time\n",
    "* Track training and validation set performance (early stopping)\n",
    "\n",
    "![Learning curves](fig/tensorboard_lstm.png)\n",
    "\n",
    "[Bengio, 2013](https://arxiv.org/abs/1206.5533)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "* http://www.deeplearningbook.org/\n",
    "* http://videolectures.net/deeplearning2016_montreal/\n",
    "* http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html\n",
    "* https://www.microsoft.com/en-us/research/video/tutorial-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's see it in practice\n",
    "\n",
    "#### Follow instructions at this link: \n",
    "\n",
    "https://github.com/bandofstraycats/reco_class\n",
    "\n",
    "#### 1. Download the SSH key.\n",
    "\n",
    "https://goo.gl/K9x9CR\n",
    "\n",
    "#### 2. Select AWS instance from the list:\n",
    "\n",
    "1. ec2-34-228-197-74.compute-1.amazonaws.com\n",
    "2. ec2-184-73-15-103.compute-1.amazonaws.com\n",
    "3. ec2-34-227-223-19.compute-1.amazonaws.com\n",
    "4. ec2-107-22-120-118.compute-1.amazonaws.com\n",
    "\n",
    "#### 2. Connect to the AWS instance using the key.\n",
    "``chmod 400 reco_class.pem\n",
    "ssh -i reco_class.pem ubuntu@AWS_INSTANCE``\n",
    "\n",
    "#### 3.  Create your working directory.\n",
    "``mkdir YOUR_NAME.YOUR_SURNAME\n",
    "cd YOUR_NAME.YOUR_SURNAME\n",
    "git clone https://github.com/bandofstraycats/reco_class.git\n",
    "cd reco_class``         \n",
    "     \n",
    "#### 3. Pick a random port from 6000 to 7000.\n",
    "\n",
    "#### 4. Start jupyter server.\n",
    "``jupyter notebook --no-browser --port YOUR_PORT --ip=*``\n",
    "\n",
    "#### 5. Open notebook in your browser.\n",
    "``http://AWS_INSTANCE:YOUR_PORT/?token=YOUR_TOKEN``\n",
    "\n",
    "#### 6. Navigate to the exercise.\n",
    "``DL_reco_class_exercise.ipynb``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "[TensorFlow](https://tensorflow.org) is an open-source machine learning library developed by Google.\n",
    "\n",
    "Tensorflow defines dataflow graph with nodes performing **mathematical operations (ops)** and edges transmitting **tensors** (multidimensional arrays). \n",
    "\n",
    "The dataflow graph represents computations that are executed within a TensorFlow session on a given device (CPUs or GPUs).\n",
    "\n",
    "![Tensorflow graph](fig/tensorflow_graph2.png)\n",
    "\n",
    "Tensorflow is able to automatically compute gradients and apply gradient-based optimization procedure.\n",
    "\n",
    "### Tensorflow programming model\n",
    "\n",
    "1. Define computational graph.\n",
    "2. Start the session.\n",
    "3. Feed the tensor(s) in the graph.\n",
    "4. Execute operations and evaluate nodes.\n",
    "\n",
    "## Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level neural networks library, running on top of either TensorFlow or Theano.\n",
    "\n",
    "Keras allows for easy and fast prototyping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wide and Deep model for Recommendation\n",
    "\n",
    "### Wide model\n",
    "* Co-counts model, memorizes co-occurences of items\n",
    "* Sparse and specific\n",
    "\n",
    "![Wide model](fig/wide_model.png)\n",
    "\n",
    "### Deep model\n",
    "* Transitivity model, generalizes from co-occurences\n",
    "* Dense and similarity-based\n",
    "\n",
    "![Deep model](fig/deep_model.png)\n",
    "\n",
    "### Wide and deep model\n",
    "* Memorizes for specific rules and generalizes for exploration\n",
    "\n",
    "![Wide and deep](fig/wide_and_deep.png)\n",
    "\n",
    "[Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Deep candidate generation\n",
    "\n",
    "![Deep candidate generation](fig/dnn-youtube.png)\n",
    "\n",
    "[Deep Neural Networks for YouTube Recommendations](https://research.google.com/pubs/archive/45530.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3: Learning Item and User Representation\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3.1: Learning Item Representation\n",
    "\n",
    "Collaborative filtering methods rely on co-occurence data that might not be available for new or niche items. The problem when no or few interaction with items are available is called _item cold-start_. This scenario is adressed by _content-based methods_ that produce recommendation based on the content of item. Indeed, in many applications, items are described by multi-modal content: image, audio, text, video.\n",
    "\n",
    "**Convolutional Neural Networks**, a particular type of deep networks, have shown to learn useful representations for image, audio and text. ConvNets have been used for content-based recommendation with encouraging results:\n",
    "\n",
    "* [Image-based recommendations on styles and substitutes](http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf)\n",
    "* [Convolutional neural networks for sentence classification](https://arxiv.org/abs/1408.5882)\n",
    "* [Deep content-based music recommendation](https://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf)\n",
    "\n",
    "![VBPR](fig/vbpr.png)\n",
    "![Music ConvNet](fig/cnn_music.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (ConvNets)\n",
    "\n",
    "___\n",
    "\n",
    "ConvNets are neural network architectures designed to work with images. In particular, they observe that recognizing an object in an image is invariant to its location. That means, the statistics over small patch of the image remain useful over the whole image, regardess the exact position.\n",
    "\n",
    "ConvNets work by moving small filters across the input image. These filters are re-used for recognizing patterns throughout the entire input image. \n",
    "\n",
    "ConvNets have been originally designed for images, but have been shown to work well with text and audio.\n",
    "\n",
    "![Image recognition](fig/image_recognition.png)\n",
    "\n",
    "![Convolution operation](fig/convolution_op_gif.gif) \n",
    "\n",
    "[CNN paper by LeCun, '98](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "**Convolution** operation convolves **filter** with the image: computer the dot product between the filter and image pixels underneath.\n",
    "\n",
    "![CNN1](fig/cnn_1.png)\n",
    "![CNN2](fig/cnn_2.png)\n",
    "![CNN3](fig/cnn_3.png)\n",
    "![CNN4](fig/cnn_4.png)\n",
    "![CNN5](fig/cnn_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution hyperparameters\n",
    "\n",
    "![Convolution hyperparameters](fig/convolution_hyperparameters.png)\n",
    "\n",
    "* input volume $W$ \n",
    "* $K$ filters, $K$-outputs = $K$ feature maps\n",
    "* receptive field $F$ = filter size \n",
    "* stride $S$ = number of pixels to shift the filter (1 = same size, 2 ~ half the size) \n",
    "* padding $P$ = what you do at the end of the image (valid, zero-padding = exactly the same size)\n",
    "\n",
    "### Example\n",
    "\n",
    "AlexNet first layer\n",
    "\n",
    "![AlexNet first layer](fig/alexnet_first_layer.png)\n",
    "\n",
    "* input image size $[227, 227, 3]$\n",
    "* $S=4, F=11, K=96, P=0$\n",
    "\n",
    "Size of the output = $(W−F+2P)/S+1 = [55\\times55\\times96]$\n",
    "\n",
    "Number of parameters: $K\\times F\\times W=96\\times11\\times11\\times3 \\approx 35 000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling\n",
    "\n",
    "The role of the pooling layer is to compress locally each features detector.\n",
    "\n",
    "Pooling operation takes max (or average) of the outputs of the convolution in a small patch of each feature map. \n",
    "\n",
    "Hyperparameters:\n",
    "* Stride\n",
    "* Size of pooling filter\n",
    "\n",
    "Advantages:\n",
    "* Increase the performance\n",
    "* Does not increase the number of parameteres to learn\n",
    "* Increase the number of hyperparameters to tune\n",
    "\n",
    "![Pooling](fig/pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stacking convolutions\n",
    "\n",
    "Stack convolution such that the output is of an increased depth: the spatial dimention is squeezed out, the semantic complexity is increasing.\n",
    "\n",
    "![Stack convolutions](fig/stack_convolutions.jpg)\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "  * Stack a pyramid of several convolutional - pooling layers with increasing the depth\n",
    "  * Connect the final deep and narrow representation to a fully connected layer\n",
    "  * 8 layers\n",
    "  \n",
    "![Alexnet features](fig/alexnet_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "* [ConvNets chapter](http://www.deeplearningbook.org/contents/convnets.html)\n",
    "* [Course at Stanford](http://cs231n.github.io/convolutional-networks/)\n",
    "* [C. Olah’s blog post](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n",
    "* [Visual intuition for ConvNets](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's see it in practice\n",
    "\n",
    "### Adding movie poster representation to deep network for recommendation\n",
    "\n",
    "* Download movie poster images\n",
    "* Apply VGG16 network to obtain movie poster features\n",
    "* Add movie poster features into deep network for recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3.2: Learning User Representation\n",
    "\n",
    "Learning representation for each userId directly is impractical for many applications because of the size of userId space. The key observation is that user is identified with a sequences of items he/she interacted with.\n",
    "\n",
    "**Recurrent Neural Network** is a special type of deep network that is designed to model sequences. Recently, RNNs have been successfully recently applied to session-based recommendation:\n",
    "\n",
    "[Session-based Recommendations with Recurrent Neural Networks](https://arxiv.org/abs/1511.06939)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "___\n",
    "\n",
    "RNNs are designed to work with sequences, such as text and speech. Importantly, sequences can be of arbitrary length.\n",
    "For example, in auto-completion task, we would like to predict the next word given previous words.\n",
    "\n",
    "![Auto-complete](fig/auto_completion.png)\n",
    "\n",
    "The input to the RNN at every time step is the **current word** as well as a **state vector** that represents the summary of what have happened so far / the past. This **state vector** is the encoded **memory** of the RNN.\n",
    "\n",
    "RNNs are usually composed of input and output connection and the recurrent cell.\n",
    "\n",
    "Unfolded RNN can be seen as very deep network in which all the layers _share the same weights_.\n",
    "\n",
    "![CNN](fig/rnn.png)\n",
    "\n",
    "RNN-based models has significantly improved performance in machine translation and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple RNN \n",
    "\n",
    "Cell is defined as an activation over the linear combination of current input and previous state.\n",
    "\n",
    "$h_t = ReLU(x_{t}W + h_{t-1}U + b)$\n",
    "\n",
    "$y_t = softmax(h_tV)$\n",
    "\n",
    "* $U$ - hidden-to-hidden weights\n",
    "* $W$ - input-to-hidden weights\n",
    "* $V$ - hidden-to-output weights\n",
    "\n",
    "![Simple RNN](fig/rnn_zoom_cell.png)\n",
    "\n",
    "### Example: character RNN\n",
    "\n",
    "![Char RNN](fig/char_rnn_explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems of training simple RNNs\n",
    "\n",
    "#### Exploding gradients\n",
    "\n",
    "Exploding gradients are problematic because we can make too large update steps. There is a simple solution that fixes it, called gradient clipping. It consists in clipping the norm of the gradient to fixed max value.\n",
    "  \n",
    "#### Vanishing gradients\n",
    "\n",
    "Vanishing gradients prevent RNN from remembering far in the past and keeping long-term dependencies. This problem motivates the new cell achitecture -- LSTM, that augment the network with an explicit memory.\n",
    "\n",
    "## Long Short-Term Memory (LSTM)\n",
    "\n",
    "![LSTM](fig/lstm_cell.png)\n",
    "\n",
    "* Internal memory $C_t$ to preserve long-term dependecies across time\n",
    "* Gates $i_t, f_t, o_t$ control how much to read, write or reset the memory using continuous functions\n",
    "  * $i_t$ controls how much of the current state is feed into the memory\n",
    "  * $f_t$ controls how much of old memory value is reset\n",
    "  * $o_t$ controls how much of memory we expose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting the continuation of the sequence\n",
    "\n",
    "A side-effect of being able to predict the next $x_{t+1}$ is that we can continue sequences one step at a time by sampling from the output probabilities.\n",
    "\n",
    "![RNN sequence generation](fig/rnn_seq_generation.png)\n",
    "\n",
    "### Procedure\n",
    "\n",
    "* Feed a word into an RNN\n",
    "* At each step, we sample $y_t$ from the output distribution and add it to the generated sequence\n",
    "* Set $x_{t+1} = y_t$ (feed the prediction as the input for the next step)\n",
    "* Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "* [RNN chapter](http://deeplearningbook.org/contents/rnn.html)\n",
    "* [Y. Bengio class](http://www.iro.umontreal.ca/~bengioy/ift6266/H16/rnn.pdf)\n",
    "* [C. Olah blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "06896fa4-0f26-4581-a5b7-b5012ed50954": {
     "id": "06896fa4-0f26-4581-a5b7-b5012ed50954",
     "prev": "d20608c9-6bf6-4894-9a96-a0b12a563cf5",
     "regions": {
      "0b53f947-9185-4abb-baf9-837901c06bed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "315a0cd0-6421-4db3-af67-eb4d1e98b8b0",
        "part": "whole"
       },
       "id": "0b53f947-9185-4abb-baf9-837901c06bed"
      }
     }
    },
    "0e63644c-6044-4c3a-98ed-6a8f9a884c9b": {
     "id": "0e63644c-6044-4c3a-98ed-6a8f9a884c9b",
     "prev": "83a03ab1-160f-4e11-9ce4-afefc948cd91",
     "regions": {
      "356f822f-9de6-4e2b-9e71-cbb6bfd97045": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e867d08d-bf18-49b9-baf2-4e36c9a9491a",
        "part": "whole"
       },
       "id": "356f822f-9de6-4e2b-9e71-cbb6bfd97045"
      }
     }
    },
    "1540be6c-a08d-4a26-8535-c1c022b9515a": {
     "id": "1540be6c-a08d-4a26-8535-c1c022b9515a",
     "prev": "0e63644c-6044-4c3a-98ed-6a8f9a884c9b",
     "regions": {
      "a574d258-5d91-4a76-b736-dfe8c8c82f19": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a4513934-a861-48b0-b213-b931cff34254",
        "part": "whole"
       },
       "id": "a574d258-5d91-4a76-b736-dfe8c8c82f19"
      }
     }
    },
    "1bdb0562-14b4-4cd4-84ea-f625ab18e6ce": {
     "id": "1bdb0562-14b4-4cd4-84ea-f625ab18e6ce",
     "prev": "284fbce4-990c-4dc7-b556-a1f13a1bf03e",
     "regions": {
      "d5bf0c71-06e2-491f-93a2-f0b58b15d5b8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3e89f08b-6254-4a4e-aaa4-b4b1de583db3",
        "part": "whole"
       },
       "id": "d5bf0c71-06e2-491f-93a2-f0b58b15d5b8"
      }
     }
    },
    "1fa683dc-7fba-43da-a3c1-af6540db4b9c": {
     "id": "1fa683dc-7fba-43da-a3c1-af6540db4b9c",
     "prev": "be6bddd3-356a-4aa1-91c2-b5ca2576182e",
     "regions": {
      "ac7a6b6d-dabd-46db-8b55-772baa672e81": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8e7e45b9-acf6-4288-a5b1-8ba3a547eae2",
        "part": "whole"
       },
       "id": "ac7a6b6d-dabd-46db-8b55-772baa672e81"
      }
     }
    },
    "284fbce4-990c-4dc7-b556-a1f13a1bf03e": {
     "id": "284fbce4-990c-4dc7-b556-a1f13a1bf03e",
     "prev": "f1e1be44-5cec-4093-bada-402cab7cbc6d",
     "regions": {
      "83c1ff58-0035-4820-b601-adb16be18f3b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6138b081-d608-48b1-923d-613889c50219",
        "part": "whole"
       },
       "id": "83c1ff58-0035-4820-b601-adb16be18f3b"
      }
     }
    },
    "2bcada8f-6285-40cb-a10d-a6437e3cfc20": {
     "id": "2bcada8f-6285-40cb-a10d-a6437e3cfc20",
     "prev": null,
     "regions": {
      "59a5b1cb-e527-4841-a9a1-64bbc35489c8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4ac6dfbf-17af-4d1b-8c0b-9d12578f3ad7",
        "part": "whole"
       },
       "id": "59a5b1cb-e527-4841-a9a1-64bbc35489c8"
      }
     }
    },
    "2cb1d1a8-b249-4ad9-9304-5063a58b878c": {
     "id": "2cb1d1a8-b249-4ad9-9304-5063a58b878c",
     "prev": "d4cc22d3-cc06-418f-9227-0a4e7d780703",
     "regions": {
      "b4ce773a-a608-4ad6-acdd-36c379b80dbd": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "52033dd9-5db2-4360-99b7-36c1a62c1219",
        "part": "whole"
       },
       "id": "b4ce773a-a608-4ad6-acdd-36c379b80dbd"
      }
     }
    },
    "3b7e2343-3648-4752-98ca-da6f8d38bbc9": {
     "id": "3b7e2343-3648-4752-98ca-da6f8d38bbc9",
     "prev": "db489ea9-a519-4004-ba79-6926fb680226",
     "regions": {
      "77a69cab-24f8-4f7d-afba-05c24820a11e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "bf1f28fe-0e39-41fd-905d-226f982962c5",
        "part": "whole"
       },
       "id": "77a69cab-24f8-4f7d-afba-05c24820a11e"
      }
     }
    },
    "48be9c4c-0172-4691-bf64-0b85d6c8b5f0": {
     "id": "48be9c4c-0172-4691-bf64-0b85d6c8b5f0",
     "prev": "9a52b869-22ba-4bfa-a1a2-319b893da4a6",
     "regions": {
      "c8d60339-3599-47d4-bcb0-f3969a317f9b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c1cadcb6-d714-40b0-9acc-708e792e8597",
        "part": "whole"
       },
       "id": "c8d60339-3599-47d4-bcb0-f3969a317f9b"
      }
     }
    },
    "6fe3fae1-b93d-4866-8be0-10e3fbba5d2e": {
     "id": "6fe3fae1-b93d-4866-8be0-10e3fbba5d2e",
     "prev": "1540be6c-a08d-4a26-8535-c1c022b9515a",
     "regions": {
      "4479ed70-9a5c-477f-a7cd-764eabad69c7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "90de1d89-350e-439a-8540-1a759de30969",
        "part": "whole"
       },
       "id": "4479ed70-9a5c-477f-a7cd-764eabad69c7"
      }
     }
    },
    "7e08cf80-b10c-4ea4-a22f-07451541803d": {
     "id": "7e08cf80-b10c-4ea4-a22f-07451541803d",
     "prev": "1bdb0562-14b4-4cd4-84ea-f625ab18e6ce",
     "regions": {
      "ab8daf7b-a827-47c7-b45e-42faee2a35fe": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "03e2dc9a-24b8-48ef-9e60-4b44faeb8657",
        "part": "whole"
       },
       "id": "ab8daf7b-a827-47c7-b45e-42faee2a35fe"
      }
     }
    },
    "83a03ab1-160f-4e11-9ce4-afefc948cd91": {
     "id": "83a03ab1-160f-4e11-9ce4-afefc948cd91",
     "prev": "ee2ebf1a-7620-42f0-a05a-f28047353a47",
     "regions": {
      "ad55e950-66ba-4d39-b268-fdba4d67320a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b5f07c68-c2a1-41d0-98be-7859ebae74d4",
        "part": "whole"
       },
       "id": "ad55e950-66ba-4d39-b268-fdba4d67320a"
      }
     }
    },
    "9a52b869-22ba-4bfa-a1a2-319b893da4a6": {
     "id": "9a52b869-22ba-4bfa-a1a2-319b893da4a6",
     "prev": "ef8770f3-c543-4a4b-98b9-77ed0c58a804",
     "regions": {
      "4d46c247-85c3-440d-af42-728d64daf7cc": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "05cadeae-26f2-4456-bc6a-6c8d3b211ab4",
        "part": "whole"
       },
       "id": "4d46c247-85c3-440d-af42-728d64daf7cc"
      }
     }
    },
    "a19fd2ef-493e-4741-a1b6-0a578b27e6ae": {
     "id": "a19fd2ef-493e-4741-a1b6-0a578b27e6ae",
     "prev": "48be9c4c-0172-4691-bf64-0b85d6c8b5f0",
     "regions": {
      "43980fbb-5506-42ee-9068-a5598df6774b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a9cafcea-51fb-4ae0-af80-cbc70ba19449",
        "part": "whole"
       },
       "id": "43980fbb-5506-42ee-9068-a5598df6774b"
      }
     }
    },
    "a61b0ff5-3ebd-4887-b528-a3f94b7dc416": {
     "id": "a61b0ff5-3ebd-4887-b528-a3f94b7dc416",
     "prev": "fba3d440-b34a-40ef-97e1-e31135a3639b",
     "regions": {
      "ddf99a61-1e05-4f20-bdfc-9d966dd136bb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "968ac6ce-eafb-4723-9eed-1db2b36f0c6a",
        "part": "whole"
       },
       "id": "ddf99a61-1e05-4f20-bdfc-9d966dd136bb"
      }
     }
    },
    "b25e0329-a528-489b-ac23-cbb1b66fb3c9": {
     "id": "b25e0329-a528-489b-ac23-cbb1b66fb3c9",
     "prev": "7e08cf80-b10c-4ea4-a22f-07451541803d",
     "regions": {
      "73bba4f3-eb59-414e-b3a9-33afb4691790": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5ad09917-aca9-4532-81e4-aeb96865fbc9",
        "part": "whole"
       },
       "id": "73bba4f3-eb59-414e-b3a9-33afb4691790"
      }
     }
    },
    "be6bddd3-356a-4aa1-91c2-b5ca2576182e": {
     "id": "be6bddd3-356a-4aa1-91c2-b5ca2576182e",
     "prev": "a19fd2ef-493e-4741-a1b6-0a578b27e6ae",
     "regions": {
      "660f8221-e561-45cc-bcab-bb3160dbee85": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ccd24e8c-817b-425f-bb4e-f27302586e69",
        "part": "whole"
       },
       "id": "660f8221-e561-45cc-bcab-bb3160dbee85"
      }
     }
    },
    "c28f9466-096b-42d2-9164-402824adc6e9": {
     "id": "c28f9466-096b-42d2-9164-402824adc6e9",
     "prev": "2bcada8f-6285-40cb-a10d-a6437e3cfc20",
     "regions": {
      "6e78aa80-e50e-4c85-8335-e63abc82eaf5": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9ea07b80-bade-4b66-9970-66dba046031c",
        "part": "whole"
       },
       "id": "6e78aa80-e50e-4c85-8335-e63abc82eaf5"
      }
     }
    },
    "cefcee13-d852-4e57-ab1a-ddaf4774d21c": {
     "id": "cefcee13-d852-4e57-ab1a-ddaf4774d21c",
     "prev": "6fe3fae1-b93d-4866-8be0-10e3fbba5d2e",
     "regions": {
      "4f91a196-3bcf-48bc-9d7b-28e74ac720c2": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec4c7e85-93af-40e5-9dcc-7c14af0c469f",
        "part": "whole"
       },
       "id": "4f91a196-3bcf-48bc-9d7b-28e74ac720c2"
      }
     }
    },
    "d08bf9b9-966b-4edd-a10d-f675b8b37b48": {
     "id": "d08bf9b9-966b-4edd-a10d-f675b8b37b48",
     "prev": "2cb1d1a8-b249-4ad9-9304-5063a58b878c",
     "regions": {
      "40361553-2702-43f4-abdc-a5b0a4994101": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d21ad19c-c0d1-4d94-bf5a-54fecf4388c5",
        "part": "whole"
       },
       "id": "40361553-2702-43f4-abdc-a5b0a4994101"
      }
     }
    },
    "d20608c9-6bf6-4894-9a96-a0b12a563cf5": {
     "id": "d20608c9-6bf6-4894-9a96-a0b12a563cf5",
     "prev": "3b7e2343-3648-4752-98ca-da6f8d38bbc9",
     "regions": {
      "2354e034-5468-4b65-a2ce-fee9179c0900": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4726501d-338c-4d75-86b7-4dd83b84103e",
        "part": "whole"
       },
       "id": "2354e034-5468-4b65-a2ce-fee9179c0900"
      }
     }
    },
    "d4cc22d3-cc06-418f-9227-0a4e7d780703": {
     "id": "d4cc22d3-cc06-418f-9227-0a4e7d780703",
     "prev": "a61b0ff5-3ebd-4887-b528-a3f94b7dc416",
     "regions": {
      "8cf5cf8c-0ee3-437a-a83f-dad74d24852e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d0a04acd-fcf4-4577-8ebd-5d13c9129a11",
        "part": "whole"
       },
       "id": "8cf5cf8c-0ee3-437a-a83f-dad74d24852e"
      }
     }
    },
    "db489ea9-a519-4004-ba79-6926fb680226": {
     "id": "db489ea9-a519-4004-ba79-6926fb680226",
     "prev": "c28f9466-096b-42d2-9164-402824adc6e9",
     "regions": {
      "20f62a49-774b-4940-9af1-9a5251fcc7e8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d5a5de58-89ee-4964-99ab-88e194f25d4a",
        "part": "whole"
       },
       "id": "20f62a49-774b-4940-9af1-9a5251fcc7e8"
      }
     }
    },
    "ee2ebf1a-7620-42f0-a05a-f28047353a47": {
     "id": "ee2ebf1a-7620-42f0-a05a-f28047353a47",
     "prev": "1fa683dc-7fba-43da-a3c1-af6540db4b9c",
     "regions": {
      "ba19e0de-855f-4644-a72f-9df9fa91f6f3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "972db601-dc8c-4e9c-b8ca-5bd20ce46d08",
        "part": "whole"
       },
       "id": "ba19e0de-855f-4644-a72f-9df9fa91f6f3"
      }
     }
    },
    "ef8770f3-c543-4a4b-98b9-77ed0c58a804": {
     "id": "ef8770f3-c543-4a4b-98b9-77ed0c58a804",
     "prev": "d08bf9b9-966b-4edd-a10d-f675b8b37b48",
     "regions": {
      "2f8f220d-c562-4cec-b9e6-0340a6489226": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "700bbe29-8cee-41b5-a99c-07d2c93d75d8",
        "part": "whole"
       },
       "id": "2f8f220d-c562-4cec-b9e6-0340a6489226"
      }
     }
    },
    "f1e1be44-5cec-4093-bada-402cab7cbc6d": {
     "id": "f1e1be44-5cec-4093-bada-402cab7cbc6d",
     "prev": "cefcee13-d852-4e57-ab1a-ddaf4774d21c",
     "regions": {
      "b0c2a0a4-d8a5-4ced-8614-fe6430fea7fa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "91c491e6-c76e-47cc-a7c0-3fd750f02b6b",
        "part": "whole"
       },
       "id": "b0c2a0a4-d8a5-4ced-8614-fe6430fea7fa"
      }
     }
    },
    "fba3d440-b34a-40ef-97e1-e31135a3639b": {
     "id": "fba3d440-b34a-40ef-97e1-e31135a3639b",
     "prev": "06896fa4-0f26-4581-a5b7-b5012ed50954",
     "regions": {
      "a9b0679a-d9e9-4728-bd9c-adda642869fd": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "47c436ce-278a-4ba1-a896-07829eaeb963",
        "part": "whole"
       },
       "id": "a9b0679a-d9e9-4728-bd9c-adda642869fd"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
