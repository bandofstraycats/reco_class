{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep learning for Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The goal of this notebook is build a recommender system for movies using deep learning on MovieLens 1M dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(url=\"https://dncache-mauganscorp.netdna-ssl.com/thumbseg/1998/1998762-bigthumbnail.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "The MovieLens 1M dataset contains 1,000,209 ratings for 3,952 movies by 6,040 users who joined MovieLens in 2000.\n",
    "If you have working locally, you can download it here: https://grouplens.org/datasets/movielens/1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_folder=\"/home/ubuntu/datasets/reco/ml-1m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_ratings = pd.read_csv(os.path.join(dataset_folder, 'ratings.dat'), sep='::', engine='python',\n",
    "                          names=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv(os.path.join(dataset_folder, 'movies.dat'), sep='::', encoding='latin-1', engine='python',\n",
    "                    names=['movieId', 'title', 'genres'])\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_ratings['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_user_id = all_ratings['userId'].max()\n",
    "max_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_item_id = all_ratings['movieId'].max()\n",
    "max_item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratings_tmp, ratings_test = train_test_split(all_ratings, test_size=0.2, random_state=0)\n",
    "\n",
    "ratings_train, ratings_validation = train_test_split(ratings_tmp, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Training set,', ratings_train.shape)\n",
    "print('Validation set,', ratings_validation.shape)\n",
    "print('Test set,', ratings_test.shape)\n",
    "\n",
    "user_id_train = ratings_train['userId']\n",
    "item_id_train = ratings_train['movieId']\n",
    "rating_train = ratings_train['rating']\n",
    "\n",
    "user_id_valid = ratings_validation['userId']\n",
    "item_id_valid = ratings_validation['movieId']\n",
    "rating_valid = ratings_validation['rating']\n",
    "\n",
    "user_id_test = ratings_test['userId']\n",
    "item_id_test = ratings_test['movieId']\n",
    "rating_test = ratings_test['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple matrix factorization\n",
    "\n",
    "Mathematically, the prediction of rating is given by $y_{ij} = u_i^T v_j$, where $u_i$ is user $i$ embedding vector, $v_j$ is item $j$ embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Flatten, merge, Dense, Dropout, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "\n",
    "embedding_size = 30\n",
    "user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "# reshape from shape: (batch_size, input_length, embedding_size)\n",
    "# to shape: (batch_size, input_length * embedding_size) which is\n",
    "# equal to shape: (batch_size, embedding_size)\n",
    "user_vecs = Flatten()(user_embedding)\n",
    "item_vecs = Flatten()(item_embedding)\n",
    "\n",
    "y = merge([user_vecs, item_vecs], mode='dot', output_shape=(1,))\n",
    "\n",
    "model = Model(input=[user_id_input, item_id_input], output=y)\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Error at random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "initial_test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Random init MSE: %0.3f\" % mean_squared_error(initial_test_preds, rating_test))\n",
    "print(\"Random init MAE: %0.3f\" % mean_absolute_error(initial_test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training\n",
    "\n",
    "Generate a batch of examples and fit the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, valid=False, batch_size=64):\n",
    "    \"\"\" batch generator to enable sparse input \"\"\"\n",
    "    index = np.arange(x[0].size)\n",
    "    start = 0\n",
    "    while True:\n",
    "        if start == 0 and valid is False:\n",
    "            np.random.shuffle(index)\n",
    "        batch = index[start:start+batch_size]\n",
    "        \n",
    "        yield [x[0].iloc[batch], x[1].iloc[batch]], y.iloc[batch]\n",
    "        \n",
    "        start += batch_size\n",
    "        if start >= x[0].size:\n",
    "            start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(get_batch([user_id_train, item_id_train], rating_train, valid=False, batch_size=64), \n",
    "                              samples_per_epoch=6400, nb_epoch=100,\n",
    "                              validation_data=get_batch([user_id_valid, item_id_valid], rating_valid, valid=True, batch_size=64), \n",
    "                              nb_val_samples=30,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Task 1: Turn simple matrix factorization into deep network\n",
    "-------\n",
    "\n",
    "Turn the simple matrix factorization into a 1-hidden layer neural network with ReLU and 64 hidden nodes. This model should improve your validation / test error. \n",
    "\n",
    "Mathematically, transform  $y_{ij} = u_i^T v_j$ into $y_{ij} = W^{(2)}(ReLU(W^{(1)}[u_i;v_j]+b^{(1)})+b^{(2)}$.\n",
    "\n",
    "To code this,\n",
    "* Concatenation of user and item vectors can be done using \"concat\" mode of `merge` function\n",
    "* Use function `Dense` to create a densely connected layer $activation(Wx + b)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(get_batch([user_id_train, item_id_train], rating_train, valid=False, batch_size=64), \n",
    "                              samples_per_epoch=6400, nb_epoch=100,\n",
    "                              validation_data=get_batch([user_id_valid, item_id_valid], rating_valid, valid=True, batch_size=64), \n",
    "                              nb_val_samples=30,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Task 2\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. \n",
    "Keras provides `Dropout` function, start with keep probability 0.5.\n",
    "\n",
    "Keep in mind that with dropout, your network is training only a part of its weights at each step, so you should increase number of epochs to have fair comparison with previous runs. For example, double number of epochs if your keep probability is 0.5.\n",
    "\n",
    "The right amount of dropout should improve your validation / test error.\n",
    "\n",
    "Mathematically, make $y_{ij} = W^{(2)}(Dropout(ReLU(W^{(1)}[u_i;v_j]+b^{(1)}), 0.5)+b^{(2)}$.\n",
    "                                       \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(get_batch([user_id_train, item_id_train], rating_train, valid=False, batch_size=64), \n",
    "                              samples_per_epoch=6400, nb_epoch=200,\n",
    "                              validation_data=get_batch([user_id_valid, item_id_valid], rating_valid, valid=True, batch_size=64), \n",
    "                              nb_val_samples=30,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Task 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! State-of-the-art methods achieve MAE of [0.68](http://www.mymedialite.net/examples/datasets.html) on MovieLens-1M.\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to try another loss function like MSE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adding item genres into the deep network\n",
    "\n",
    "Meta information about movies might help prediction, for example, in the case of cold-start when few ratings are available about item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert genres of items from text into vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "genre_to_id = { \"Action\":0, \"Adventure\":1, \"Animation\":2, \"Children's\":3, \"Comedy\":4, \"Crime\":5, \"Documentary\":6, \"Drama\":7, \n",
    "               \"Fantasy\":8, \"Film-Noir\":9, \"Horror\":10, \"Musical\":11, \"Mystery\": 12, \"Romance\":13, \"Sci-Fi\":14, \"Thriller\":15, \n",
    "               \"War\":16, \"Western\":17}    \n",
    "genre_ids = []\n",
    "for genre in items['genres']:\n",
    "    genre_vector = (np.arange(18)==19).astype(np.float32)\n",
    "    for genre_str in genre.split('|'):\n",
    "        genre_vector += (np.arange(18) == genre_to_id[genre_str]).astype(np.float32)\n",
    "    genre_ids.append(genre_vector)\n",
    "    \n",
    "items['genre_ids'] = pd.Series(genre_ids, index=items.index)\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "Task 5\n",
    "---------\n",
    "\n",
    "Modify the network definition below to include `meta_input` vector that represents a vector of item genres.\n",
    "\n",
    "Train the network with the procedure below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "meta_input = Input(shape=[18,], name='meta_item')\n",
    "\n",
    "embedding_size = 32\n",
    "user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "user_vecs = Flatten()(user_embedding)\n",
    "item_vecs = Flatten()(item_embedding)\n",
    "\n",
    "input_vecs = merge([user_vecs, item_vecs], mode='concat')\n",
    "\n",
    "h1 = Dense(64, activation='relu')(input_vecs)\n",
    "\n",
    "h1_dropout = Dropout(0.5)(h1)\n",
    "\n",
    "y = Dense(1)(h1_dropout)\n",
    "\n",
    "model = Model(input=[user_id_input, item_id_input, meta_input], output=y)\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_join_batch(x, y, z, valid=False, batch_size=64):\n",
    "    \"\"\" batch generator to enable sparse input \"\"\"\n",
    "    index = np.arange(x[0].size)\n",
    "    start = 0\n",
    "    while True:\n",
    "        if start == 0 and valid is False:\n",
    "            np.random.shuffle(index)\n",
    "        batch = index[start:start+batch_size]\n",
    "        merge_x_z = pd.merge(x[1].iloc[batch].to_frame(), z, on='movieId')\n",
    "        yield [x[0].iloc[batch], x[1].iloc[batch], np.matrix(merge_x_z['genre_ids'].tolist())], y.iloc[batch]\n",
    "        start += batch_size\n",
    "        if start >= x[0].size:\n",
    "            start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(get_join_batch([user_id_train, item_id_train], rating_train, items[['movieId', 'genre_ids']], \n",
    "                                        valid=False, batch_size=64), \n",
    "                              samples_per_epoch=6400, nb_epoch=200,\n",
    "                              validation_data=get_join_batch([user_id_valid, item_id_valid], rating_valid, items[['movieId', 'genre_ids']], \n",
    "                                                        valid=True, batch_size=64), \n",
    "                              nb_val_samples=30,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merge_x_z = pd.merge(item_id_test.to_frame(), items[['movieId', 'genre_ids']], on='movieId')\n",
    "test_preds = model.predict([user_id_test, item_id_test, np.matrix(merge_x_z['genre_ids'].tolist())])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding image features using pre-trained VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(url='https://blog.keras.io/img/imgclf/vgg16_original.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of movie poster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_folder = '/home/ubuntu/datasets/reco/images_movie_1m/all'\n",
    "Image(filename=images_folder + '/1.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieIds = dict()\n",
    "for dirpath, dirnames, filenames in os.walk(images_folder):\n",
    "    for f in filenames:\n",
    "        movieIds[int(f.split('.')[0])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed features: VGG16 output on movie posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 1000\n",
    "image_features = np.load(open('/home/ubuntu/datasets/reco/images_movie_1m_features.npy', 'rb')).reshape((3800, size))\n",
    "image_features_df = pd.concat([pd.Series(movieIds.keys(), dtype=np.int32), pd.Series(image_features.tolist())], axis=1, keys=['movieId', 'image_features'])\n",
    "for i in range(max_item_id+1):\n",
    "    if i not in movieIds:\n",
    "        df_tmp = pd.concat([pd.Series([i], dtype=np.int32), pd.Series([np.zeros((size,), dtype=np.float32)])], axis=1, keys=['movieId', 'image_features'])\n",
    "        image_features_df = pd.concat([image_features_df, df_tmp], ignore_index=True)\n",
    "image_features_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Task 6\n",
    "---------\n",
    "\n",
    "Modify the network definition to include images features vector that is the output of VGG16 network of size 1000.\n",
    "\n",
    "Train the network with the procedure below. Modify the network definition to get the best accuracy you can.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_join_batch(x, y, z, valid=False, batch_size=64):\n",
    "    \"\"\" batch generator to enable sparse input \"\"\"\n",
    "    index = np.arange(x[0].size)\n",
    "    start = 0\n",
    "    while True:\n",
    "        if start == 0 and valid is False:\n",
    "            np.random.shuffle(index)\n",
    "        batch = index[start:start+batch_size]\n",
    "        merge_x_z = pd.merge(x[1].iloc[batch].to_frame(), z, on='movieId')\n",
    "        yield [x[0].iloc[batch], x[1].iloc[batch], np.array(merge_x_z['image_features'].tolist(), dtype=np.float32)], y.iloc[batch]\n",
    "        start += batch_size\n",
    "        if start >= x[0].size:\n",
    "            start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(get_join_batch([user_id_train, item_id_train], rating_train, image_features_df, \n",
    "                                        valid=False, batch_size=64), \n",
    "                              samples_per_epoch=6400, nb_epoch=200,\n",
    "                              validation_data=get_join_batch([user_id_valid, item_id_valid], rating_valid, image_features_df, \n",
    "                                                        valid=True, batch_size=64), \n",
    "                              nb_val_samples=30,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_x_z = pd.merge(item_id_test.to_frame(), image_features_df, on='movieId')\n",
    "test_preds = model.predict([user_id_test, item_id_test, np.matrix(merge_x_z['image_features'].tolist())])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
